Understanding Machine Learning with the Iris Dataset

Table of Contents???
# Introduction
# Setup
# Data Curation, Parsing, and Management
# Exploratory Data Analysis
TODO
# Hypothesis Testing & Machine Learning for Analysis
TODO
# Insights
TODO???

Here's the iris csv

From now on I'll just post Rmarkdown contents directly. Ctrl+c Ctrl+v everything and you have yourself an rmarkdown document.

---
title: "Understanding Machine Learning with the Iris Dataset"
output: html_document
---
### Maya Fuchs, Steve Moore & Shiyu Hao
#### CMSC320 Spring 2019

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

  The Iris Flower Dataset is a classic when it comes to learning data science. There are plenty of tutorials for statistical analysis and machine learning using the dataset, but it's not always easy to understand why all the steps are required, and it can be harder to understand how to apply them to new contexts. 

  In this tutorial, we'll take you through data processing, analysis, hypothesis testing and machine learning with the iris dataset. The goal here is to build up a strong intuition for the various steps in the process, so it will be easier to translate to harder problems. After all, most datasets we end up using won't be as simple and straightforward as this one.

# Setup

Here are the libraries we'll need. Be sure to install any that you don't already have!

``` {r libraries, message=FALSE, warning=FALSE}
library(neuralnet)
library(readr)
library(rvest)
library(ggplot2)
library(nnet)
library(dplyr)
library(reshape2)
```
# Data Curation, Parsing, and Management

The Iris dataset is already built into R, so to load it we simply call:

``` {r iris}
data("iris")
```
Not every dataset is that simple, so let's try another method: loading CSV files.

We've included the iris csv file in our git repository. You can view the raw data in your browser, right click and select "save as," name it and save it as a csv in a local directory.

``` {r csv}
# All you really need for a simple dataset like this:
# iris_csv <- read_csv("iris.csv")

# But if the column types are less obvious, explicitly assign column types
iris_csv <- read_csv("iris.csv",
                     col_types = cols(sepal_length = col_double(),
                                      sepal_width = col_double(),
                                      petal_length = col_double(),
                                      petal_width = col_double(),
                                      species = col_character()))

# Print part of the data to make sure the types are correct and everything looks good.
head(iris_csv)
```

That's my favorite method, since it tends to be fairly simple. You can specify column types in the call to read_csv or after the fact, rename columns, etc. It's also very easy to clean the data in excel or google sheets before loading it, then a single call like the one above gives you a dataset that's ready to go.


# Exploratory Data Analysis
TODO [talk about eda stuff here]
TODO [Other visualization examples]

The best way to approach visualization is to look at the dataset, see which variables are interesting, then google examples of plots for inspiration. To apply them to a new dataset, start simple and add details one at a time, adjusting as you go along. Here we'll go over a few creative approaches to visualizing the classic iris dataset.

Here's an example of a fancy boxplot I found here: https://www.r-exercises.com/2017/11/17/iris-neural-network-solutions/

``` {r eda}
exploratory_iris <- melt(iris)
exploratory_iris %>%
  ggplot(aes(x = factor(variable), y = value)) +
  geom_violin() +
  geom_jitter(height = 0, width = 0.1, aes(colour = Species), alpha = 0.7) +
  theme_minimal()
```

This graph lets us see the distributions of widths/heights across each of the 3 species. We can see a bit of distinction in terms of petal length and width - the setosas have much smaller petals; the other two are more similar, but the virginicas are somewhat bigger than the versicolors. 

TODO [more analysis of plots]

What about relationships between variables? This page: http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs has a series of plots ranging from simple:

```{r facetgrid}
pairs(iris[,1:4], pch = 19)
```

... to detailed and colorful:

``` {r colorfulgrid}
my_cols <- c("#00AFBB", "#E7B800", "#FC4E07")  
pairs(iris[,1:4], pch = 19,  cex = 0.5,
      col = my_cols[iris$Species],
      lower.panel=NULL)

# Correlation panel
panel.cor <- function(x, y){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- round(cor(x, y), digits=2)
    txt <- paste0("R = ", r)
    cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
# Customize upper panel
upper.panel<-function(x, y){
  points(x,y, pch = 19, col = my_cols[iris$Species])
}
# Create the plots
pairs(iris[,1:4], 
      lower.panel = panel.cor,
      upper.panel = upper.panel)
```

... to elaborate:

``` {r fancygrid}
library(psych)
pairs.panels(iris[,-5], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

The last plot shows bivariate scatter plots with regression lines and correlation ellipses on the bottom left, histograms on the diagonal, and pearson correlation on the top right. The color-coded plot above it shows the distributions for each class better, but the last plot does a better job of concretely displaying variable relationships and overall distributions. It might be best to start with simpler plots, but this elaborate version makes the relationship characteristics very clear.

There seems to be a strong linear relationship between petal length and width, but the other variables are not linearly related. Sepal length does seem to have a pretty strong correlation with petal length and width, however.

The individual distributions of petal length and width seem to be bimodal - there are two main clusters, with about 2/3 of the data in one, and 1/3 in the other. We can confirm this in the violin plot above, and can see that the clusters correspond to the species: versicolor and virginica are one group, and setosa is the other.

This dataset is a perfect classification problem, given the three species and simple set of characteristics to distinguish them. The question is, which model do we use to predict the species of a given flower? 

Since one pair of variables has a clear linear relationship, we could try to fit a linear model to the data.
We could probably separate setosa from the other two species easily enough, but we might not be able to separate versicolor and virginica as easily.

We could use logistic regression to predict the probabilities of a given flower belonging to each class. 

Logistic regression to predict species:

Suppose we are given an unknown species of plant. We know that its species could either be virginica or versicolor based on its measurements. We can fit a logistic regression over our data to estimate the probability of our unknown's species being virginica or versicolor. 


``` {r logisticRegressionModel}

iris_no_setosa <- filter(iris, Species %in% c("virginica", "versicolor"))
glm_model <- glm(Species ~ Sepal.Width + Sepal.Length + Petal.Width + Petal.Length,
               data = iris_no_setosa,
               family = binomial) # family = binomial required for logistic regression
summary(glm_model)

model_data <- data.frame(predictor=glm_model$linear.predictors, prob=glm_model$fitted.values, Species=iris_no_setosa$Species)
ggplot(model_data, aes(x=predictor, y=prob, color=Species)) + geom_point()
```

Above we have fitted a linear regression model to determine the probability of an unknown plant being a versicolor or a virginica. To make a prediction of what a unknown plant is, we will plug in the predictor attributes into our logistic model and then get the probability or our unknown being a versicolor.


``` {r logisticRegressionPrediction}
unknown_plant <- data.frame(Sepal.Length=5.4, Sepal.Width=2.4, Petal.Length=4.1, Petal.Width=1.9)
predict(glm_model, unknown_plant, type="response")
```

Here we have created an unknown plant so we can get an idea of its species by plugging it into our logistic model. We gave our model a plant with 

Sepal length = 5.4 
Sepal width = 2.4
Petal length = 4.1
Petal width = 1.9

and our model states this unknown plant has a 0.8053412% chance of being of the versicolor species.


We could use K-nearest neighbors to group the flowers based on similarities. 
KNN algorithm making estimations and predictions
A common situation in data analysis is that one has an outcome attribute, and one or more independent covariate or predictor attributes. We can ask ourselves some questions about these attributes. What effects do the covariates have on the outcome? How well can we describe these effects? Can we predict the outcome using the covariates?

We can do some data analysis with geometry to help give us some answers.

Here, We can select species as our outcome attribute, petal length, petal width, and sepal width as our predictor attributes. We suspect there might be a linear relationship between the attributes. Suppose we want to predict what a species will be given certain petal length and width? we can use the K-nearest-neighbor classification to give us an estimate. Given petal length,and  petal and sepal widths, we could estimate what species group the new plant falls into.

The k nearest neighbors is a algorithm that looks at all available normalized data and classifies new data by looking at its k neighbors. This algorithms segregates unlabeled data points into well defined groups.

What value should we select for k? A large k value is good because it will not be affected by data noise as much, but it might overlook the less noticeable patterns in the data.

``` {r KNN}
 library(class)
 iris_data <- iris %>% select(Petal.Length,Petal.Width, Sepal.Width, Species)

 ##the normalization function is created
 nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
 ##Run normalization on first 3 columns of dataset because they are the predictors
 iris_norm <- as.data.frame(lapply(iris_data[,c(1,2,3)], nor))
 
 #Creating hypothetical new data entry by modifying existing one
 test_data <- iris_norm[3,]
 test_data[1,1] <-  test_data[1,1] + .2
 test_data[1,2] <-  test_data[1,2] + .08
 test_data[1,3] <-  test_data[1,3] + .14
 test_data <- test_data %>% as_tibble()

 ##run knn function
 estimate <- knn(iris_norm,test_data,cl=iris_data[,4],k=13)
 head(iris_data)
 test_data
 estimate
```


Here, we have our dataset where we only care about our outcome and predictor attributes. We created a new data entry by modifying an original one. We inputted the new data entry into our knn algorithm to see what our estimate would be. Our test data was 

Petal length = 0.2508475
Petal width = 0.1216667
Sepal width = 0.64

and our knn algorithm predicted these attributes correspond to the setosa species. Which lines up with the rest of the observations nicely. This would lead us to believe a plant with these attributes would indeed be part of the Setosa species.


We could use a decision tree, where we'd determine certain thresholds for characteristics. For example, from the violin plot we can see that setosa flowers don't have petal widths > 1. We could let the machine try to find enough clear distinctions to accurately separate the data. 

Decision trees tend to have a problem with overfitting, so their results are often difficult to generalize to new data. They can also require very large datasets to achieve some degree of reliability. These may or may not be issues with this dataset, but they certainly could hinder results for more complicated ones. A popular solution is the random forest model, which creates multiple decision trees, each of which select random features to focus on. Then, the results from the trees are basically averaged to make a final decision.

TODO [more details and/or other models]




# Machine Learning for Analysis- 





[logistic regression (classification)] - DONE
[Decision tree] - DONE
[Random Forest]
# Insights
TODO


# Resources
https://www.r-exercises.com/2017/11/17/iris-neural-network-exercises/ 
http://www.hcbravo.org/IntroDataSci/bookdown-notes/index.html 
http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs


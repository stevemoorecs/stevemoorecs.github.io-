#Understanding Machine Learning with the Iris Dataset

Iris dataset: https://drive.google.com/file/d/1hSkoIbRG7BFe78jdqnQRiagU3C-sDlUJ/view?usp=sharing

---
output: html_document
---

### Maya Fuchs, Steve Moore & Shiyu Hao
#### CMSC320 Spring 2019

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

  The Iris Flower Dataset is a classic when it comes to learning data science. There are plenty of tutorials for statistical analysis and machine learning using the dataset, but it's not always easy to understand why all the steps are required, and it can be harder to understand how to apply them to new contexts. 

  In this tutorial, we'll take you through data processing, analysis, hypothesis testing and machine learning with the iris dataset. The goal here is to build up a strong intuition for the various steps in the process, so it will be easier to translate to harder problems. After all, most datasets we end up using won't be as simple and straightforward as this one.

# Setup

Here are the libraries we'll need. Be sure to install any that you don't already have!

``` {r libraries, message=FALSE, warning=FALSE}
library(neuralnet)
library(readr)
library(rvest)
library(ggplot2)
library(nnet)
library(dplyr)
library(reshape2)
library(magrittr)
library(class)
library(broom)
```
The familiar dplyr and readr libraries offer tools for working with dataframes and rectangular data. Ggplot2 is essential for what we do with visualizing datasets, and reshape2 for certain data transformations.

We will be using rvest and magrittr for a short demonstration of scraping data from websites.

The neuralnet and nnet will cover what we need for neural networks in the machine learning part.

# Data Curation, Parsing, and Management

The Iris dataset is already built into R, so to load it we simply call:

``` {r iris}
data("iris")
```
Not every dataset is that simple, so let's try another method: loading CSV files.

We've included the iris csv file in our git repository. You can view the raw data in your browser, right click and select "save as," name it and save it as a csv in a local directory.

``` {r csv}
# All you really need for a simple dataset like this:
# iris_csv <- read_csv("iris.csv")

# But if the column types are less obvious, explicitly assign column types
iris_csv <- read_csv("iris.csv",
                     col_types = cols(sepal_length = col_double(),
                                      sepal_width = col_double(),
                                      petal_length = col_double(),
                                      petal_width = col_double(),
                                      species = col_character()))

# Print part of the data to make sure the types are correct and everything looks good.
head(iris_csv)
```

That's my favorite method, since it tends to be fairly simple. You can specify column types in the call to read_csv or after the fact, rename columns, etc. It's also very easy to clean the data in excel or google sheets before loading it, then a single call like the one above gives you a dataset that's ready to go.

If you are looking to scrape your own data from a website, there are a few extra steps.

First, locate a website with desired data. We use https://www.weather.com with its 10 day weather forecast for College Park, MD. If you are unfamiliar with the basics of html, feel free to refer to this page: https://www.w3schools.com/html/html_basic.asp

``` {r scraping}
# Loads url
url <- "https://weather.com/weather/tenday/l/20740:4:US"
# Extract html data. In this case the data is placed inside a table node, which can be identified with the "table" id. Examine the html for the data structure's id if you don't already know it.

# The extracted node is placed inside a list. So we use .[[1]] to access it as the only element inside.

# The fill parameter is set to TRUE because the original html table assigns the first column to NA's. This will require some post-processing
weather_table <- url %>% 
          read_html() %>% 
          html_nodes("table") %>% 
          .[[1]] %>% 
          html_table(fill = TRUE)

# Clean up data
weather_table$Day <- weather_table$description

weather_table <- weather_table %>% set_colnames(c("Day", 
                                    "Description", 
                                    "High/Low", "Precip", 
                                    "Wind", "Humidity"))
``` 

This will result in a data frame that can be examined or transformed in the same way loading a csv file would create!

For a more detailed walkthrough, this tutorial from DataCamp is a great resource:  
https://www.datacamp.com/community/tutorials/r-web-scraping-rvest


# Exploratory Data Analysis

Exploratory data analysis, as the name suggests, is the initial analysis of a dataset through exploration. It examines variable properties and relationships, which provide insights into the statistical models or machine learning techniques we should choose later on. If there is still any need for data transformation, exploratory data analysis is also a last check. 

We will cover two main perspectives for exploratory data analysis here - visualization, and summary statistics.

Visualization is a great starting point for conducting EDA. It offers direct and intuitive knowledge about the dataset. R also supports a wide range of graphics options, and we use the ggplot2 library mentioned above. For more information regarding ggplot2, you may reference its official documentation: https://ggplot2.tidyverse.org

The best way to approach visualization is to look at the dataset, see which variables are interesting, then google examples of plots for inspiration. To apply them to a new dataset, start simple and add details one at a time, adjusting as you go along. Here we'll go over a few creative approaches to visualizing the classic iris dataset.

Here's an example of a fancy boxplot I found here: https://www.r-exercises.com/2017/11/17/iris-neural-network-solutions/

``` {r eda}
exploratory_iris <- melt(iris)
exploratory_iris %>%
  ggplot(aes(x = factor(variable), y = value)) +
  geom_violin() +
  geom_jitter(height = 0, width = 0.1, aes(colour = Species), alpha = 0.7) +
  theme_minimal()
```

This graph lets us see the distributions of widths/heights across each of the 3 species. We can see a bit of distinction in terms of petal length and width - the setosas have much smaller petals; the other two are more similar, but the virginicas are somewhat bigger than the versicolors. 

A graph like this easily allows us to see some immediate features or trends within our data. It gives an overview of what exact aspects of each single variable are worth further analysis. At this point, since our data is quite simply distributed with identifiable patterns, we can move on to summary statistics as described later in this section to quantify these observations.

What about relationships between variables? This page: http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs has a series of plots ranging from simple:

```{r facetgrid}
pairs(iris[,1:4], pch = 19)
```

... to detailed and colorful:

``` {r colorfulgrid}
my_cols <- c("#00AFBB", "#E7B800", "#FC4E07")  
pairs(iris[,1:4], pch = 19,  cex = 0.5,
      col = my_cols[iris$Species],
      lower.panel=NULL)

# Correlation panel
panel.cor <- function(x, y){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- round(cor(x, y), digits=2)
    txt <- paste0("R = ", r)
    cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
# Customize upper panel
upper.panel<-function(x, y){
  points(x,y, pch = 19, col = my_cols[iris$Species])
}
# Create the plots
pairs(iris[,1:4], 
      lower.panel = panel.cor,
      upper.panel = upper.panel)
```

... to elaborate:

``` {r fancygrid}
library(psych)
pairs.panels(iris[,-5], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

The last plot shows bivariate scatter plots with regression lines and correlation ellipses on the bottom left, histograms on the diagonal, and pearson correlation on the top right. The color-coded plot above it shows the distributions for each class better, but the last plot does a better job of concretely displaying variable relationships and overall distributions. It might be best to start with simpler plots, but this elaborate version makes the relationship characteristics very clear.

There seems to be a strong linear relationship between petal length and width, but the other variables are not linearly related. Sepal length does seem to have a pretty strong correlation with petal length and width, however.

The individual distributions of petal length and width seem to be bimodal - there are two main clusters, with about 2/3 of the data in one, and 1/3 in the other. We can confirm this in the violin plot above, and can see that the clusters correspond to the species: versicolor and virginica are one group, and setosa is the other.

We have gathered quite an amount of visual clues simply by examining the graphs. As our dataset is fairly straight forward, it would be more or less enough for us to poceed to the next step of our machine learning case study, which is fitting our dataset to a statistical model or machine learning method. In other cases, this might be a good place to turn to summary statistics as another means to continue EDA. 

Summary statistics alludes to the familiar names mean, median, max, etc. They are the set of numbers that summarizes the characteristics of a dataset. Concretely knowing these measurements would allow us to have a firmer grasp on the distribution tendencies of our data. It also provides a foundation for us to transform our data, such as carrying out standardization, if it is needed.

Here is some code that shows how to obtain the five point summary (median, max, min, lower and upper fourth quartiles) for sepal lengths of versicolors.

``` {r statistics}
# There are individual functions in R that computes many statistical values
iris_stat <- iris_csv %>% 
  filter(species=="versicolor") %>%
  summarize(min = min(sepal_length), 
            max = max(sepal_length), 
            median = median(sepal_length), 
            lower_fourth = quantile(sepal_length, 0.25), 
            upper_fourth = quantile(sepal_length, 0.75))

# There is also a compact function that carries out all above computations
summary(filter(iris_csv, species=="versicolor")$sepal_length)
```

Since our goal today is to carry you through the entire procedure of machine learning based on the iris dataset, we won't dwell into too much details regarding R and statistics. CRAN has a more comprehensive overview revolving around the qwraps2 graphics library for extracting such quantities, as well as building summary tables that will be handy for a statistics-focused analysis here: https://cran.r-project.org/web/packages/qwraps2/vignettes/summary-statistics.html.

We can also combine the values that we have obtained with visualization techniques.

``` {r stat&visual}
# Piping the dataset into a simple histogram and then graphing lines that indicate the mean, upper and lower quartiles respectively.
iris_csv %>%
  filter(species=="versicolor") %>%
  ggplot(aes(x=sepal_length)) +
    geom_histogram(bins=20) +
    geom_vline(aes(xintercept=mean(sepal_length)), size=1.3, color="red") +
    geom_vline(aes(xintercept=quantile(sepal_length, 0.25)), size=0.8, color="blue", linetype=2) +
    geom_vline(aes(xintercept=quantile(sepal_length, 0.75)), size=0.8, color="blue", linetype=2)
```

Each method will prove to be more or less useful according to the dataset that is used. In our case, the necessity for computing and plotting the specific statistic values is low since there is no outstanding need for data transformation or careful evaluation between certain models given the relationship between variables, as we will see later on. Nevertheless, it is helpful to keep in mind that there are many options available when exploring a dataset. The majority of real-world data can be messy, with many complications that can be best dealt with by diverse means.

In many cases, we may wish to standardize our values into a unit-less format that can eliminate certain noises. This would also allow direct  cross-comparison between variables that are measured in different units. 

``` {r standardize}
# Using the standardization formula to create a new variable of standardized sepal length variable. The result would be a new variable that can be seen as measured in "number of standard deviations away from mean".
standard_sepal_length <- iris_csv %>%
  filter(species=="versicolor") %>%
  mutate(standard_var=(sepal_length-mean(sepal_length))/sd(sepal_length))

standard_sepal_length$standard_var
```

We might also have to transform the way that our variable exists in the data frame, from numeric to categorical, or the other way around. Since we will be predicting the species of a certain iris given measurement parameters, this also would not be necessary in our case. The motivation for doing so is because most prediction methods, including linear regression which will be used shortly, only take numerical inputs. If we want to use species or other categorical descriptions to predict an output through linear regression, we would have to do something like this...

``` {r type_transform}
# This method is known as one-hot encoding. If an entity is a certain species the column corresponding to such species will be 1, otherwise it will be 0.
type_trans <- iris_csv %>%
  mutate(species_versicolor=ifelse(species=="versicolor", 1, 0), 
         species_virginica=ifelse(species=="virginica", 1, 0),
         species_setosa=ifelse(species=="setosa", 1, 0))
```

For more information regarding transformations, refer to CRAN's documentation:
https://cran.r-project.org/web/packages/dlookr/vignettes/transformation.html

This dataset is a perfect classification problem, given the three species and simple set of characteristics to distinguish them. The question is, which model do we use to predict the species of a given flower? 

Since one pair of variables has a clear linear relationship, we could try to fit a linear model to the data.
We could probably separate setosa from the other two species easily enough, but we might not be able to separate versicolor and virginica as easily.

Sometimes when looking at your data, you can see a pattern between attributes but can't quite identify it over the noise of background information. We can use hypothesis testing combined with linear regression to help us determine if the patterns we see are truly significant. First we need to determine our threshold of significance. That is, assuming this is a random sample of data, what is the probability of observing the data in this sample? What probability means there is a relationship between attributes and not just random coincidences occurring? Generally, if we observe a probability, that we observe this data, of less than or equal to 5%, we can say that there are no coincidences and the relationships in our data are significant. For our purposes, we will use this general threshold

Suppose I wanted to see if there was a relationship between petal length and sepal length. We can look at a scatter plot of the two attributes.

```{r linearRegression1}

data(iris)

iris %>%
  select(Petal.Length, Sepal.Length) %>%
  ggplot(mapping=aes(x=Petal.Length,y=Sepal.Length)) + geom_point() + 
   labs(title="Scatter Plot: Sepal Length over Petal Length",
         x = "Petal Length",
         y = "Sepal Length")
```

Above we see a scatter plot of petal length over sepal length. There does indeed to appear to be a relationship between the two attributes. Now we can make a hypothesis about the relationship between petal and sepal length. In statistics, we make what is called a null hypothesis. Here, our null hypothesis would be that there is NO significant relationship between petal and sepal lengths. If our p-value, or probability of observing our data, is less than or equal to 5% we can safely REJECT this hypothesis and say that there IS a relationship between sepal length and width. Since our relationship looks linear, we can fit a linear regression model to our data to check our p-value.


```{r linearRegression2}
data(iris)
length_fit <- lm(Petal.Length~Sepal.Length, data=iris) %>% tidy()

length_fit
```
Now we can see that our p-value is basically zero, so there is a very low chance of observing this relationship between petal and sepal length randomly. Thus, we reject the null hypothesis and can indeed say there exists a relationship between petal and sepal length. 

Below I have included a linear regression line on top of the scatter plot from earlier so the relationship is more clear. 

```{r linearRegression3}
data(iris)
iris %>%
  select(Petal.Length, Sepal.Length) %>%
  ggplot(mapping=aes(x=Petal.Length,y=Sepal.Length)) + geom_point() + 
  geom_smooth(method = lm) +
   labs(title="Scatter Plot: Petal Length over Sepal Length (Regression Line Fitted)",
         x = "Petal Length",
         y = "Sepal Length")
```

# Machine Learning for Analysis

We could use logistic regression to predict the probabilities of a given plant belonging to each class. 

Logistic regression to predict species:

Suppose we are given an unknown species of plant. We know that its species could either be virginica or versicolor based on its measurements. We can fit a logistic regression over our data to estimate the probability of our unknown's species being virginica or versicolor. 


``` {r logisticRegressionModel}
iris_no_setosa <- filter(iris, Species %in% c("virginica", "versicolor"))
glm_model <- glm(Species ~ Sepal.Width + Sepal.Length + Petal.Width + Petal.Length,
               data = iris_no_setosa,
               family = binomial) # family = binomial required for logistic regression
summary(glm_model)

model_data <- data.frame(predictor=glm_model$linear.predictors, prob=glm_model$fitted.values, Species=iris_no_setosa$Species)
ggplot(model_data, aes(x=predictor, y=prob, color=Species)) + geom_point()
```

Above we have fitted a logistic regression model to determine the probability of an unknown plant being a versicolor or a virginica. To make a prediction of what a unknown plant is, we will plug in the predictor attributes into our logistic model and then get the probability or our unknown being a versicolor.


``` {r logisticRegressionPrediction}
unknown_plant <- data.frame(Sepal.Length=5.4, Sepal.Width=2.4, Petal.Length=4.1, Petal.Width=1.9)
predict(glm_model, unknown_plant, type="response")
```

Here we have created an unknown plant so we can get an idea of its species by plugging it into our logistic model. We gave our model a plant with 

Sepal length = 5.4 
Sepal width = 2.4
Petal length = 4.1
Petal width = 1.9

and our model states this unknown plant has a 0.8053412% chance of being of the versicolor species.

There are many other ways to estimate attributes like this. 
We could use K-nearest neighbors to estimate and group the flowers based on similarities. 
KNN algorithm making estimations and predictions: 

A common situation in data analysis is that one has an outcome attribute, and one or more independent covariate or predictor attributes. We can ask ourselves some questions about these attributes. What effects do the covariates have on the outcome? How well can we describe these effects? Can we predict the outcome using the covariates?

We can do some data analysis with geometry to help give us some answers.

Suppose we want to predict what a species will be given certain petal length, petal and sepal width. Here, We can select species as our outcome attribute, followed by petal length, petal width, and sepal width as our predictor attributes. We can use the K-nearest-neighbor classification to give us an estimate. Given petal length, and petal and sepal widths, we could estimate what species group the new plant falls into.

The k nearest neighbors is an algorithm that looks at all available normalized data and classifies new data by looking at its k neighbors. This algorithms segregates unlabeled data points into well defined groups.

What value should we select for k? A large k value is good because it will not be affected by data noise as much, but it might overlook the less noticeable patterns in the data.

``` {r KNN}
 iris_data <- iris %>% select(Petal.Length,Petal.Width, Sepal.Width, Species)

 ##the normalization function is created
 nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
 ##Run normalization on first 3 columns of dataset because they are the predictors
 iris_norm <- as.data.frame(lapply(iris_data[,c(1,2,3)], nor))
 
 #Creating hypothetical new data entry by modifying existing one
 test_data <- iris_norm[3,]
 test_data[1,1] <-  test_data[1,1] + .2
 test_data[1,2] <-  test_data[1,2] + .08
 test_data[1,3] <-  test_data[1,3] + .14
 test_data <- test_data %>% as_tibble()

 ##run knn function
 estimate <- knn(iris_norm,test_data,cl=iris_data[,4],k=13)
 head(iris_data)
 test_data
 estimate
```


Here, we have our dataset where we only care about our outcome and predictor attributes. We created a new data entry by modifying an original one. We inputted the new data entry into our knn algorithm to see what our estimate would be. Our test data was 

Petal length = 0.2508475
Petal width = 0.1216667
Sepal width = 0.64

and our knn algorithm predicted these attributes correspond to the setosa species. Which lines up with the rest of the observations nicely. This would lead us to believe a plant with these attributes would indeed be part of the Setosa species.

We could use a decision tree:

We could use a decision tree to determine certain thresholds for characteristics. For example, from the violin plot we can see that setosa flowers don't have petal widths > 1. We could let the machine try to find enough clear distinctions to accurately separate the data. 

Decision trees tend to have a problem with overfitting, so their results are often difficult to generalize to new data. They can also require very large datasets to achieve some degree of reliability. These may or may not be issues with this dataset, but they certainly could hinder results for more complicated ones. A popular solution is the random forest model, which creates multiple decision trees, each of which select random features to focus on. Then, the results from the trees are basically averaged to make a final decision.

# Insights

The iris dataset is used very frequently in data science, but the motivation for using it in specific ways can be confusing at times. Our goal with this project was to have a variety of approaches in one place, to make it easier to compare and understand the purpose for choosing one method over another for a given problem. It's easier to learn difficult concepts starting with the basics, so hopefully we've managed to keep things fairly simple. 

To me, visualization seemed pretty trivial compared to the massively exciting concept of machine learning. In hindsight, I think it's one of the most useful tools for making sense of things that don't otherwise make sense. The more complicated a dataset or a given question is, the harder it is to see what is going on. The iris dataset is a walk in the park. With a solid grasp on visualizing and organizing data, we can start to navigate what can often be a tangled conceptual, mathematical, or practical mess. These skills are truly invaluable, and are undeniably the foundation for any good data science. Machine learning itself can get a bit harder. Start with regression and work your way up little by little, playing around with the iris dataset as you go. Once the basics are down, all that's left for new problems is trial and error - you can test out different models to see which works for any dataset. 


# Resources

https://www.r-exercises.com/2017/11/17/iris-neural-network-exercises/ 
http://www.hcbravo.org/IntroDataSci/bookdown-notes/index.html 
http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs
https://www.w3schools.com/html/html_basic.asp
https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
https://ggplot2.tidyverse.org
https://www.r-exercises.com/2017/11/17/iris-neural-network-solutions/
https://youtu.be/ZzWaow1Rvho 
https://cran.r-project.org/web/packages/qwraps2/vignettes/summary-statistics.html
https://cran.r-project.org/web/packages/dlookr/vignettes/transformation.html
